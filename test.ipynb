{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0681c42",
   "metadata": {},
   "source": [
    "### Notes On Sat_Dat Tensorbuilding\n",
    "\n",
    "- Two different possible storage formats\n",
    "    - Var > Time > Lat > Long > Data Point\n",
    "        - Easier to build\n",
    "        - Likely larger size\n",
    "        - Probably faster runtime\n",
    "    - Time > Lat > Long > [Var1, Var2, Var3]\n",
    "        - Harder to build\n",
    "        - Will require organizing vars in memory for efficiency\n",
    "            - Make data structure, put parts in bit by bit\n",
    "        - Will likely take much longer to run\n",
    "        \n",
    "\n",
    "- Multithreading would be great\n",
    "    - Set queue of all variables to process\n",
    "    - Grab oldest var from queue\n",
    "        - process data\n",
    "        - put into individual var_named file\n",
    "        - If failure, put variable back into queue\n",
    "        \n",
    "        \n",
    "        \n",
    "- GFS is a forecast, I don't want to be forecasting a forecast probably.  Could still be useful as a back up or for validation, but now I'm looking for non-forecast waveheight measurements\n",
    "- Running into issues with ftp servers, sticking with GFS predictions.  Model is now more of a translator than predictor\n",
    "- How to deal with updates, get new data daily and overwrite existing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2020e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, sys, time, datetime, urllib, ftplib, os\n",
    "import pandas as pd\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "url='http://nomads.ncep.noaa.gov:80/dods/wave/gfswave/20220305/gfswave.epacif.0p16_00z'\n",
    "url='http://nomads.ncep.noaa.gov:80/dods/wave/gfswave/20220305/gfswave.wcoast.0p16_00z'\n",
    "url='http://nomads.ncep.noaa.gov:80/dods/wave/gfswave/20220305/gfswave.gsouth.0p25_12z' # This one has data [0,0]\n",
    "url='http://nomads.ncep.noaa.gov:80/dods/wave/gfswave/20220305/gfswave.gsouth.0p25_18z' # This one has data [100,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00e1e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = ftplib.ftp_get(host, product_id, dataset_id, user, password)\n",
    "# https://sealevel.jpl.nasa.gov/data/get-data/ sketchy data \n",
    "\n",
    "# # da = xr.open_dataset(url)\n",
    "# # da['dirpwsfc']\n",
    "# host = \"nrt.cmems-du.eu\"\n",
    "# user = \"rwitt\"\n",
    "# password = \"guGpa7-myzfug-syvhus\"\n",
    "# product_id = \"WAVE_GLO_WAV_L3_SPC_NRT_OBSERVATIONS_014_002\"\n",
    "# dataset_id = 'cmems_obs-wave_glo_phy-spc_nrt_cfo-l3_PT10S'\n",
    "# file = 'ftp://nrt.cmems-du.eu/Core/WAVE_GLO_WAV_L3_SPC_NRT_OBSERVATIONS_014_002/cmems_obs-wave_glo_phy-spc_nrt_cfo-l3_PT10S'\n",
    "\n",
    "\n",
    "# with ftplib.FTP(host) as ftp:\n",
    "#     ftp.login(user, password)\n",
    "#     ftp.dir()\n",
    "#     ftp.cwd('/Core')\n",
    "#     ftp.cwd('WAVE_GLO_WAV_L3_SWH_NRT_OBSERVATIONS_014_001')\n",
    "#     ftp.dir()\n",
    "#     ftp.cwd('dataset-wav-alti-l3-swh-rt-global-al')\n",
    "#     ftp.dir()\n",
    "#     ftp.cwd('2022')\n",
    "#     ftp.dir()\n",
    "#     ftp.cwd('01')\n",
    "#     ftp.dir()\n",
    "# #     with open('/Users/rowenwitt/downloads/test', 'wb') as fp:\n",
    "            \n",
    "# #             res = ftp.retrbinary('RETR ' + 'global_vavh_l3_rt_al_20220131T180000_20220131T210000_20220201T000134.nc', fp.write)\n",
    "    \n",
    "# with open('/Users/rowenwitt/downloads/test', 'rb') as f:\n",
    "#     dataset = []\n",
    "#     for line in f:\n",
    "#         dataset.append(json.loads(line))\n",
    "\n",
    "\n",
    "### \n",
    "# Eu once daily record data \n",
    "# test = Dataset(url)\n",
    "# test = Dataset('https://nrt.cmems-du.eu/thredds/dodsC/dataset-wav-l4-swh-nrt-global') # once daily ESA SWH product\n",
    "\n",
    "\n",
    "# len(test.variables['time'])\n",
    "# test.variables['time'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "170c1d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def build_tensor_threaded(url):\n",
    "    '''takes netCDF4 url and converts into a tensor'''\n",
    "    to_avoid = {'time', 'lat', 'lon'}\n",
    "    ds = Dataset(url)\n",
    "    total_timesteps = len(ds.variables['time'])\n",
    "    queue = []\n",
    "    refresh = False\n",
    "    logger = []\n",
    "    \n",
    "    dims = {}\n",
    "    for key in ds.dimensions.keys():\n",
    "        dims[key] = ds.dimensions[key].size\n",
    "    \n",
    "    variables = {}\n",
    "    for num, value in enumerate(ds.variables.keys()):\n",
    "        variables[num] = value\n",
    "        \n",
    "    c = 0\n",
    "    while len(queue) > 0:    # queue of timesteps\n",
    "        next_up = queue.pop(0)\n",
    "        try:\n",
    "            if refresh == True:\n",
    "                ds  = Dataset(url)\n",
    "            timestep = []\n",
    "            for variable in variables:\n",
    "                c += 1\n",
    "                if variables[variable] in to_avoid:\n",
    "                    if variables[variable] == 'time':\n",
    "                        print(\"Starting timestep {}/{}.  SubIter {} of {}.\".format(time+1, total_timesteps,c ,((len(variables) - 1) * total_timesteps)))\n",
    "#                         if (time + 1) % 20 == 0:\n",
    "#                             ds = Dataset(url)\n",
    "#                             print('refreshing connection')\n",
    "    #                     ### DEBUG ###\n",
    "    #                     elif (time + 1) % 10 == 0:\n",
    "    #                         return outset, variables, to_avoid\n",
    "    #                     #############\n",
    "                    continue\n",
    "                else:\n",
    "                    var_data = []\n",
    "                    foc = ds[variables[variable]][time] # variable\n",
    "                    timestep.append(foc.data)\n",
    "            outset.append(timestep)\n",
    "        except Exception as e:\n",
    "            logger.append('error {}'.format(e))\n",
    "            refresh = True\n",
    "            ds = Dataset(url)\n",
    "            queue.insert(0, next_up)\n",
    "            logger.append('restarting {}'.format(next_up))\n",
    "    \n",
    "    outset = np.array(outset)\n",
    "    outset[outset == 9.999000260554009e+20] = 0\n",
    "    return outset, variables, to_avoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "52e909d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting timestep 1/129.  SubIter 1 of 2709.\n",
      "Starting timestep 2/129.  SubIter 23 of 2709.\n",
      "Starting timestep 3/129.  SubIter 45 of 2709.\n",
      "Starting timestep 4/129.  SubIter 67 of 2709.\n",
      "Starting timestep 5/129.  SubIter 89 of 2709.\n",
      "Starting timestep 6/129.  SubIter 111 of 2709.\n",
      "Starting timestep 7/129.  SubIter 133 of 2709.\n",
      "Starting timestep 8/129.  SubIter 155 of 2709.\n",
      "Starting timestep 9/129.  SubIter 177 of 2709.\n",
      "Starting timestep 10/129.  SubIter 199 of 2709.\n",
      "Starting timestep 11/129.  SubIter 221 of 2709.\n",
      "Starting timestep 12/129.  SubIter 243 of 2709.\n",
      "Starting timestep 13/129.  SubIter 265 of 2709.\n",
      "Starting timestep 14/129.  SubIter 287 of 2709.\n",
      "Starting timestep 15/129.  SubIter 309 of 2709.\n",
      "Starting timestep 16/129.  SubIter 331 of 2709.\n",
      "Starting timestep 17/129.  SubIter 353 of 2709.\n",
      "Starting timestep 18/129.  SubIter 375 of 2709.\n",
      "Starting timestep 19/129.  SubIter 397 of 2709.\n",
      "Starting timestep 20/129.  SubIter 419 of 2709.\n",
      "refreshing connection\n",
      "Starting timestep 21/129.  SubIter 441 of 2709.\n",
      "Starting timestep 22/129.  SubIter 463 of 2709.\n",
      "Starting timestep 23/129.  SubIter 485 of 2709.\n",
      "Starting timestep 24/129.  SubIter 507 of 2709.\n",
      "Starting timestep 25/129.  SubIter 529 of 2709.\n",
      "Starting timestep 26/129.  SubIter 551 of 2709.\n",
      "Starting timestep 27/129.  SubIter 573 of 2709.\n",
      "Starting timestep 28/129.  SubIter 595 of 2709.\n",
      "Starting timestep 29/129.  SubIter 617 of 2709.\n",
      "Starting timestep 30/129.  SubIter 639 of 2709.\n",
      "Starting timestep 31/129.  SubIter 661 of 2709.\n",
      "Starting timestep 32/129.  SubIter 683 of 2709.\n",
      "Starting timestep 33/129.  SubIter 705 of 2709.\n",
      "Starting timestep 34/129.  SubIter 727 of 2709.\n",
      "Starting timestep 35/129.  SubIter 749 of 2709.\n",
      "Starting timestep 36/129.  SubIter 771 of 2709.\n",
      "Starting timestep 37/129.  SubIter 793 of 2709.\n",
      "Starting timestep 38/129.  SubIter 815 of 2709.\n",
      "Starting timestep 39/129.  SubIter 837 of 2709.\n",
      "Starting timestep 40/129.  SubIter 859 of 2709.\n",
      "refreshing connection\n",
      "Starting timestep 41/129.  SubIter 881 of 2709.\n",
      "Starting timestep 42/129.  SubIter 903 of 2709.\n",
      "Starting timestep 43/129.  SubIter 925 of 2709.\n",
      "Starting timestep 44/129.  SubIter 947 of 2709.\n",
      "Starting timestep 45/129.  SubIter 969 of 2709.\n",
      "Starting timestep 46/129.  SubIter 991 of 2709.\n",
      "Starting timestep 47/129.  SubIter 1013 of 2709.\n",
      "Starting timestep 48/129.  SubIter 1035 of 2709.\n",
      "Starting timestep 49/129.  SubIter 1057 of 2709.\n",
      "Starting timestep 50/129.  SubIter 1079 of 2709.\n",
      "Starting timestep 51/129.  SubIter 1101 of 2709.\n",
      "Starting timestep 52/129.  SubIter 1123 of 2709.\n",
      "Starting timestep 53/129.  SubIter 1145 of 2709.\n",
      "Starting timestep 54/129.  SubIter 1167 of 2709.\n",
      "Starting timestep 55/129.  SubIter 1189 of 2709.\n",
      "Starting timestep 56/129.  SubIter 1211 of 2709.\n",
      "Starting timestep 57/129.  SubIter 1233 of 2709.\n",
      "Starting timestep 58/129.  SubIter 1255 of 2709.\n",
      "Starting timestep 59/129.  SubIter 1277 of 2709.\n",
      "Starting timestep 60/129.  SubIter 1299 of 2709.\n",
      "refreshing connection\n",
      "Starting timestep 61/129.  SubIter 1321 of 2709.\n",
      "Starting timestep 62/129.  SubIter 1343 of 2709.\n",
      "Starting timestep 63/129.  SubIter 1365 of 2709.\n",
      "Starting timestep 64/129.  SubIter 1387 of 2709.\n",
      "Starting timestep 65/129.  SubIter 1409 of 2709.\n",
      "Starting timestep 66/129.  SubIter 1431 of 2709.\n",
      "Starting timestep 67/129.  SubIter 1453 of 2709.\n",
      "Starting timestep 68/129.  SubIter 1475 of 2709.\n",
      "Starting timestep 69/129.  SubIter 1497 of 2709.\n",
      "Starting timestep 70/129.  SubIter 1519 of 2709.\n",
      "Starting timestep 71/129.  SubIter 1541 of 2709.\n",
      "Starting timestep 72/129.  SubIter 1563 of 2709.\n",
      "Starting timestep 73/129.  SubIter 1585 of 2709.\n",
      "Starting timestep 74/129.  SubIter 1607 of 2709.\n",
      "Starting timestep 75/129.  SubIter 1629 of 2709.\n",
      "Starting timestep 76/129.  SubIter 1651 of 2709.\n",
      "Starting timestep 77/129.  SubIter 1673 of 2709.\n",
      "Starting timestep 78/129.  SubIter 1695 of 2709.\n",
      "Starting timestep 79/129.  SubIter 1717 of 2709.\n",
      "Starting timestep 80/129.  SubIter 1739 of 2709.\n",
      "refreshing connection\n",
      "Starting timestep 81/129.  SubIter 1761 of 2709.\n",
      "Starting timestep 82/129.  SubIter 1783 of 2709.\n",
      "Starting timestep 83/129.  SubIter 1805 of 2709.\n",
      "Starting timestep 84/129.  SubIter 1827 of 2709.\n",
      "Starting timestep 85/129.  SubIter 1849 of 2709.\n",
      "Starting timestep 86/129.  SubIter 1871 of 2709.\n",
      "Starting timestep 87/129.  SubIter 1893 of 2709.\n",
      "Starting timestep 88/129.  SubIter 1915 of 2709.\n",
      "Starting timestep 89/129.  SubIter 1937 of 2709.\n",
      "Starting timestep 90/129.  SubIter 1959 of 2709.\n",
      "Starting timestep 91/129.  SubIter 1981 of 2709.\n",
      "Starting timestep 92/129.  SubIter 2003 of 2709.\n",
      "Starting timestep 93/129.  SubIter 2025 of 2709.\n",
      "Starting timestep 94/129.  SubIter 2047 of 2709.\n",
      "Starting timestep 95/129.  SubIter 2069 of 2709.\n",
      "Starting timestep 96/129.  SubIter 2091 of 2709.\n",
      "Starting timestep 97/129.  SubIter 2113 of 2709.\n",
      "Starting timestep 98/129.  SubIter 2135 of 2709.\n",
      "Starting timestep 99/129.  SubIter 2157 of 2709.\n",
      "Starting timestep 100/129.  SubIter 2179 of 2709.\n",
      "refreshing connection\n",
      "Starting timestep 101/129.  SubIter 2201 of 2709.\n",
      "Starting timestep 102/129.  SubIter 2223 of 2709.\n",
      "Starting timestep 103/129.  SubIter 2245 of 2709.\n",
      "Starting timestep 104/129.  SubIter 2267 of 2709.\n",
      "Starting timestep 105/129.  SubIter 2289 of 2709.\n",
      "Starting timestep 106/129.  SubIter 2311 of 2709.\n",
      "Starting timestep 107/129.  SubIter 2333 of 2709.\n",
      "Starting timestep 108/129.  SubIter 2355 of 2709.\n",
      "Starting timestep 109/129.  SubIter 2377 of 2709.\n",
      "Starting timestep 110/129.  SubIter 2399 of 2709.\n",
      "Starting timestep 111/129.  SubIter 2421 of 2709.\n",
      "Starting timestep 112/129.  SubIter 2443 of 2709.\n",
      "Starting timestep 113/129.  SubIter 2465 of 2709.\n",
      "Starting timestep 114/129.  SubIter 2487 of 2709.\n",
      "Starting timestep 115/129.  SubIter 2509 of 2709.\n",
      "Starting timestep 116/129.  SubIter 2531 of 2709.\n",
      "Starting timestep 117/129.  SubIter 2553 of 2709.\n",
      "Starting timestep 118/129.  SubIter 2575 of 2709.\n",
      "Starting timestep 119/129.  SubIter 2597 of 2709.\n",
      "Starting timestep 120/129.  SubIter 2619 of 2709.\n",
      "refreshing connection\n",
      "Starting timestep 121/129.  SubIter 2641 of 2709.\n",
      "Starting timestep 122/129.  SubIter 2663 of 2709.\n",
      "Starting timestep 123/129.  SubIter 2685 of 2709.\n",
      "Starting timestep 124/129.  SubIter 2707 of 2709.\n",
      "Starting timestep 125/129.  SubIter 2729 of 2709.\n",
      "Starting timestep 126/129.  SubIter 2751 of 2709.\n",
      "Starting timestep 127/129.  SubIter 2773 of 2709.\n",
      "Starting timestep 128/129.  SubIter 2795 of 2709.\n",
      "Starting timestep 129/129.  SubIter 2817 of 2709.\n",
      "CPU times: user 1min 7s, sys: 41.8 s, total: 1min 49s\n",
      "Wall time: 59min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# [1, x, x] = time\n",
    "# [x, 1, x] = latitude\n",
    "# [x, x, 1] = longitude\n",
    "# dirpwsfc # surface primary wave direction [deg] \n",
    "# htsgwsfc # surface significant height of combined wind waves and swell [m]\n",
    "# perpwsfc # surface primary wave mean period [s] \n",
    "# swdir_1  # 1 in sequence direction of swell waves [deg] \n",
    "# swdir_2  # 2 in sequence direction of swell waves [deg] \n",
    "# swdir_3  # 3 in sequence direction of swell waves [deg]\n",
    "# swper_1  # 1 in sequence mean period of swell waves [s] \n",
    "# swper_2  # 2 in sequence mean period of swell waves [s]\n",
    "# swper_3  # 3 in sequence mean period of swell waves [s]\n",
    "# ugrdsfc  # surface u-component of wind [m/s] \n",
    "# vgrdsfc  # surface v-component of wind [m/s] \n",
    "# wdirsfc  # surface wind direction (from which blowing) [degtrue] \n",
    "# windsfc  # surface wind speed [m/s] \n",
    "# wvdirsfc # surface direction of wind waves [deg] \n",
    "# wvhgtsfc # surface significant height of wind waves [m] \n",
    "# wvpersfc # surface mean period of wind waves [s]\n",
    "\n",
    "# Each array has 129 times\n",
    "# In each time subarray theres 277 latitudes\n",
    "# In each time-latitude subarray theres 1440 longitudes\n",
    "\n",
    "# Latitudes below 45 might not have data\n",
    "\n",
    "def build_tensor(url):\n",
    "    '''takes netCDF4 url and converts into a tensor'''\n",
    "    to_avoid = {'time':0, 'lat':1, 'lon':2}\n",
    "    ds = Dataset(url)\n",
    "    total_timesteps = len(ds.variables['time'])\n",
    "    outset = []\n",
    "    \n",
    "    dims = {}\n",
    "    for key in ds.dimensions.keys():\n",
    "        dims[key] = ds.dimensions[key].size\n",
    "    \n",
    "    variables = {}\n",
    "    for num, value in enumerate(ds.variables.keys()):\n",
    "        variables[num] = value\n",
    "        \n",
    "    c = 0\n",
    "    for time in range(dims['time']): # time\n",
    "        timestep = []\n",
    "        for variable in variables:\n",
    "            c += 1\n",
    "            if variables[variable] in to_avoid:\n",
    "                if variables[variable] == 'time':\n",
    "                    print(\"Starting timestep {}/{}.  SubIter {} of {}.\".format(time+1, total_timesteps,c ,((len(variables) - 1) * total_timesteps)))\n",
    "                    if (time + 1) % 20 == 0:\n",
    "                        ds = Dataset(url)\n",
    "                        print('refreshing connection')\n",
    "#                     ### DEBUG ###\n",
    "#                     elif (time + 1) % 10 == 0:\n",
    "#                         return outset, variables, to_avoid\n",
    "#                     #############\n",
    "                continue\n",
    "            else:\n",
    "                var_data = []\n",
    "                foc = ds[variables[variable]][time] # variable\n",
    "                timestep.append(foc.data)\n",
    "        outset.append(timestep)\n",
    "    \n",
    "    outset = np.array(outset)\n",
    "    outset[outset == 9.999000260554009e+20] = 0\n",
    "    return outset, variables, to_avoid\n",
    " \n",
    "# Data collection keeps erroring out, need a solution to this.  Understand opendap and how to grab as much data\n",
    "# Ask quickly as possible, multithreading, chunking, etc\n",
    "data, variables, avoided = build_tensor(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Threaded test ###\n",
    "data_2 = build_tensor_threaded(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "14a9b21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3910.61968"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(data)  * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a314a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_tensor_alt(tensor, variable_order):\n",
    "    tensor = np.array(tensor)\n",
    "    starting_size = sys.getsizeof(tensor)\n",
    "    shape = tensor.shape\n",
    "    out_tensor = np.array(shape)\n",
    "    finish_flag = False\n",
    "    errors = []\n",
    "#     temp_hash = []\n",
    "    test_hash = []\n",
    "    for timestep in range(shape[0]):\n",
    "        var_hash = {}\n",
    "        for variable in range(shape[1]):\n",
    "            lat_arr = []\n",
    "            if variable == shape[1] - 1:\n",
    "                finish_flag = True\n",
    "            for latitude in range(shape[2]):\n",
    "                latitude = str(latitude)\n",
    "                if var_hash.get(latitude) == None:\n",
    "                    var_hash[latitude] = {}\n",
    "                lon_arr = []\n",
    "                for longitude in range(shape[3]):\n",
    "                    longitude = str(longitude)\n",
    "                    if var_hash[latitude].get(longitude) == None:\n",
    "                        var_hash[latitude][longitude] = {}\n",
    "                    if var_hash.get(latitude, {}).get(longitude, {}).get(variable_order[variable]):\n",
    "                        errors.append(\"error var:{} lat:{} long:{} time:{}\".format(variable_order[variable]))\n",
    "                    else:\n",
    "                        var_hash[latitude][longitude][variable_order[variable]] = tensor[timestep][variable][int(latitude)][int(longitude)]\n",
    "                        if finish_flag:\n",
    "                            lon_arr.append(np.array(list(var_hash[latitude][longitude].values()), dtype=np.float16))\n",
    "                                           \n",
    "                if finish_flag:\n",
    "                    lat_arr.append(np.array(lon_arr, dtype=np.float16))\n",
    "        if finish_flag:\n",
    "            test_hash.append(np.array(lat_arr, dtype=np.float16))\n",
    "            print(\"finished {} of {}\".format(timestamp, shape[0]))\n",
    "#         temp_hash.append(var_hash)\n",
    "        # Once verified that all data is there, need to convert to array dict\n",
    "        \n",
    "        \n",
    "        \n",
    "    return test_hash, errors\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db8ddd70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f359c3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to decrease size of tensor, ~4 gbs per file currently\n",
    "# Do triple loop to go through each orientation, grab each individual data point, organize it\n",
    "# time > variable > lat > lon > data\n",
    "# time > lat > lon > [var1, var2, var3] ## NEW\n",
    "\n",
    "# To shrink the tensor, the options are either to learn topology (grab whole neighborhoods )\n",
    "\n",
    "# variable_order = list(set(variables.values()) - set(avoided))\n",
    "variable_order = list(x.keys())\n",
    "external = ''\n",
    "def shrink_tensor(tensor, variable_order):\n",
    "    tensor = np.array(tensor)\n",
    "    starting_size = sys.getsizeof(tensor)\n",
    "    shape = tensor.shape\n",
    "    finish_flag = False\n",
    "    errors = []\n",
    "    test_hash = []\n",
    "    for timestep in range(shape[0]):\n",
    "        var_hash = {}\n",
    "        for variable in range(shape[1]):\n",
    "            lat_arr = []\n",
    "            if variable == shape[1] - 1:\n",
    "                finish_flag = True\n",
    "            for latitude in range(shape[2]):\n",
    "                latitude = str(latitude)\n",
    "                if var_hash.get(latitude) == None:\n",
    "                    var_hash[latitude] = {}\n",
    "                lon_arr = []\n",
    "                for longitude in range(shape[3]):\n",
    "                    longitude = str(longitude)\n",
    "                    if var_hash[latitude].get(longitude) == None:\n",
    "                        var_hash[latitude][longitude] = {}\n",
    "                    if var_hash.get(latitude, {}).get(longitude, {}).get(variable_order[variable]):\n",
    "                        errors.append(\"error var:{} lat:{} long:{} time:{}\".format(variable_order[variable]))\n",
    "                    else:\n",
    "                        var_hash[latitude][longitude][variable_order[variable]] = tensor[timestep][variable][int(latitude)][int(longitude)]\n",
    "                        if finish_flag:\n",
    "                            lon_arr.append(list(var_hash[latitude][longitude].values()))\n",
    "                                           \n",
    "                if finish_flag:\n",
    "                    lat_arr.append(np.array(lon_arr, dtype=np.float16))\n",
    "        if finish_flag:\n",
    "            print(sys.getsizeof(test_hash) ** 1e-6)\n",
    "            test_hash.append(np.array(lat_arr, dtype=np.float16))\n",
    "            print(\"finished {} of {}\".format(timestep, shape[0]))\n",
    "        \n",
    "        \n",
    "        \n",
    "    return test_hash, errors\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3127b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del data\n",
    "data = load_file('nparray', 'pull_one.npy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7dcd37c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.65"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][10][100][0]\n",
    "# data[0][10][100][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ebe785a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277, 1440, 19)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b39ba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.65"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][100][0][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d5e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000040253597924\n",
      "finished 0 of 129\n",
      "1.0000044773468377\n",
      "finished 1 of 129\n",
      "1.0000044773468377\n",
      "finished 2 of 129\n",
      "1.0000044773468377\n",
      "finished 3 of 129\n",
      "1.0000044773468377\n",
      "finished 4 of 129\n",
      "1.0000047875032028\n",
      "finished 5 of 129\n",
      "1.0000047875032028\n",
      "finished 6 of 129\n",
      "1.0000047875032028\n",
      "finished 7 of 129\n",
      "1.0000047875032028\n",
      "finished 8 of 129\n",
      "1.0000052149493555\n",
      "finished 9 of 129\n",
      "1.0000052149493555\n",
      "finished 10 of 129\n",
      "1.0000052149493555\n",
      "finished 11 of 129\n",
      "1.0000052149493555\n",
      "finished 12 of 129\n",
      "1.0000052149493555\n",
      "finished 13 of 129\n",
      "1.0000052149493555\n",
      "finished 14 of 129\n",
      "1.0000052149493555\n",
      "finished 15 of 129\n",
      "1.0000052149493555\n",
      "finished 16 of 129\n",
      "1.0000055134439452\n",
      "finished 17 of 129\n",
      "1.0000055134439452\n",
      "finished 18 of 129\n",
      "1.0000055134439452\n",
      "finished 19 of 129\n",
      "1.0000055134439452\n",
      "finished 20 of 129\n",
      "1.0000055134439452\n",
      "finished 21 of 129\n",
      "1.0000055134439452\n",
      "finished 22 of 129\n",
      "1.0000055134439452\n",
      "finished 23 of 129\n",
      "1.0000055134439452\n",
      "finished 24 of 129\n",
      "1.000005743019679\n",
      "finished 25 of 129\n",
      "1.000005743019679\n",
      "finished 26 of 129\n",
      "1.000005743019679\n",
      "finished 27 of 129\n",
      "1.000005743019679\n",
      "finished 28 of 129\n",
      "1.000005743019679\n",
      "finished 29 of 129\n",
      "1.000005743019679\n",
      "finished 30 of 129\n",
      "1.000005743019679\n",
      "finished 31 of 129\n",
      "1.000005743019679\n",
      "finished 32 of 129\n",
      "1.0000059296067234\n",
      "finished 33 of 129\n",
      "1.0000059296067234\n",
      "finished 34 of 129\n",
      "1.0000059296067234\n",
      "finished 35 of 129\n",
      "1.0000059296067234\n",
      "finished 36 of 129\n",
      "1.0000059296067234\n",
      "finished 37 of 129\n",
      "1.0000059296067234\n",
      "finished 38 of 129\n",
      "1.0000059296067234\n",
      "finished 39 of 129\n",
      "1.0000059296067234\n",
      "finished 40 of 129\n",
      "1.0000061569979397\n",
      "finished 41 of 129\n",
      "1.0000061569979397\n",
      "finished 42 of 129\n",
      "1.0000061569979397\n",
      "finished 43 of 129\n",
      "1.0000061569979397\n",
      "finished 44 of 129\n",
      "1.0000061569979397\n",
      "finished 45 of 129\n",
      "1.0000061569979397\n",
      "finished 46 of 129\n",
      "1.0000061569979397\n",
      "finished 47 of 129\n",
      "1.0000061569979397\n",
      "finished 48 of 129\n",
      "1.0000061569979397\n",
      "finished 49 of 129\n",
      "1.0000061569979397\n",
      "finished 50 of 129\n",
      "1.0000061569979397\n",
      "finished 51 of 129\n",
      "1.0000061569979397\n",
      "finished 52 of 129\n",
      "1.00000634214153\n",
      "finished 53 of 129\n",
      "1.00000634214153\n",
      "finished 54 of 129\n",
      "1.00000634214153\n",
      "finished 55 of 129\n",
      "1.00000634214153\n",
      "finished 56 of 129\n",
      "1.00000634214153\n",
      "finished 57 of 129\n",
      "1.00000634214153\n",
      "finished 58 of 129\n",
      "1.00000634214153\n",
      "finished 59 of 129\n",
      "1.00000634214153\n",
      "finished 60 of 129\n",
      "1.00000634214153\n",
      "finished 61 of 129\n",
      "1.00000634214153\n",
      "finished 62 of 129\n",
      "1.00000634214153\n",
      "finished 63 of 129\n",
      "1.00000634214153\n",
      "finished 64 of 129\n",
      "1.0000064983032633\n",
      "finished 65 of 129\n",
      "1.0000064983032633\n",
      "finished 66 of 129\n",
      "1.0000064983032633\n",
      "finished 67 of 129\n",
      "1.0000064983032633\n",
      "finished 68 of 129\n",
      "1.0000064983032633\n",
      "finished 69 of 129\n",
      "1.0000064983032633\n",
      "finished 70 of 129\n",
      "1.0000064983032633\n",
      "finished 71 of 129\n",
      "1.0000064983032633\n",
      "finished 72 of 129\n",
      "1.0000064983032633\n",
      "finished 73 of 129\n",
      "1.0000064983032633\n",
      "finished 74 of 129\n",
      "1.0000064983032633\n",
      "finished 75 of 129\n",
      "1.0000064983032633\n",
      "finished 76 of 129\n",
      "1.0000066745836667\n",
      "finished 77 of 129\n",
      "1.0000066745836667\n",
      "finished 78 of 129\n",
      "1.0000066745836667\n",
      "finished 79 of 129\n",
      "1.0000066745836667\n",
      "finished 80 of 129\n",
      "1.0000066745836667\n",
      "finished 81 of 129\n",
      "1.0000066745836667\n",
      "finished 82 of 129\n",
      "1.0000066745836667\n",
      "finished 83 of 129\n",
      "1.0000066745836667\n",
      "finished 84 of 129\n",
      "1.0000066745836667\n",
      "finished 85 of 129\n",
      "1.0000066745836667\n",
      "finished 86 of 129\n",
      "1.0000066745836667\n",
      "finished 87 of 129\n",
      "1.0000066745836667\n",
      "finished 88 of 129\n",
      "1.0000066745836667\n",
      "finished 89 of 129\n",
      "1.0000066745836667\n",
      "finished 90 of 129\n",
      "1.0000066745836667\n",
      "finished 91 of 129\n",
      "1.0000066745836667\n",
      "finished 92 of 129\n",
      "1.0000068243969562\n",
      "finished 93 of 129\n",
      "1.0000068243969562\n",
      "finished 94 of 129\n",
      "1.0000068243969562\n",
      "finished 95 of 129\n",
      "1.0000068243969562\n",
      "finished 96 of 129\n",
      "1.0000068243969562\n",
      "finished 97 of 129\n",
      "1.0000068243969562\n",
      "finished 98 of 129\n",
      "1.0000068243969562\n",
      "finished 99 of 129\n",
      "1.0000068243969562\n",
      "finished 100 of 129\n",
      "1.0000068243969562\n",
      "finished 101 of 129\n",
      "1.0000068243969562\n",
      "finished 102 of 129\n",
      "1.0000068243969562\n",
      "finished 103 of 129\n",
      "1.0000068243969562\n",
      "finished 104 of 129\n",
      "1.0000068243969562\n",
      "finished 105 of 129\n",
      "1.0000068243969562\n",
      "finished 106 of 129\n",
      "1.0000068243969562\n",
      "finished 107 of 129\n",
      "1.0000068243969562\n",
      "finished 108 of 129\n",
      "1.0000069847407134\n",
      "finished 109 of 129\n",
      "1.0000069847407134\n",
      "finished 110 of 129\n",
      "1.0000069847407134\n",
      "finished 111 of 129\n",
      "1.0000069847407134\n",
      "finished 112 of 129\n",
      "1.0000069847407134\n",
      "finished 113 of 129\n",
      "1.0000069847407134\n",
      "finished 114 of 129\n",
      "1.0000069847407134\n",
      "finished 115 of 129\n",
      "1.0000069847407134\n",
      "finished 116 of 129\n",
      "1.0000069847407134\n",
      "finished 117 of 129\n",
      "1.0000069847407134\n",
      "finished 118 of 129\n",
      "1.0000069847407134\n",
      "finished 119 of 129\n",
      "1.0000069847407134\n",
      "finished 120 of 129\n",
      "1.0000069847407134\n",
      "finished 121 of 129\n",
      "1.0000069847407134\n",
      "finished 122 of 129\n",
      "1.0000069847407134\n",
      "finished 123 of 129\n",
      "1.0000069847407134\n",
      "finished 124 of 129\n",
      "1.0000069847407134\n",
      "finished 125 of 129\n",
      "1.0000069847407134\n",
      "finished 126 of 129\n",
      "1.0000069847407134\n",
      "finished 127 of 129\n",
      "1.0000069847407134\n",
      "finished 128 of 129\n",
      "CPU times: user 1h 8min 24s, sys: 1min 12s, total: 1h 9min 37s\n",
      "Wall time: 1h 10min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = load_file('nparray', 'pull_one.npy')\n",
    "f = np.array(data)\n",
    "del data\n",
    "\n",
    "ftest, errors = shrink_tensor(f, variable_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bfcb9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d46ae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 4.77 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def save_file(file, filetype, filename='test'):\n",
    "    root_path = os.path.expanduser('~/downloads')\n",
    "    if filetype == 'nparray':\n",
    "#         np.savetxt(root_path+filename, file)\n",
    "        np.save(root_path+'/'+filename+'.npy', data)\n",
    "    elif filetype == 'netcdf':\n",
    "        pass\n",
    "    \n",
    "def load_file(filetype, filename):\n",
    "    if filetype == 'nparray':\n",
    "        file = np.load(os.path.expanduser('~/downloads') +'/'+ filename, allow_pickle=True)\n",
    "    \n",
    "    return file\n",
    "# save_file(data, filetype='nparray', filename='pull_one')\n",
    "# data = load_file('nparray', 'pull_one.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803ee8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = {'perpwsfc': 277.29,\n",
    " 'swdir_2': 5.19,\n",
    " 'swper_1': 11.759999,\n",
    " 'ugrdsfc': 9.999e+20,\n",
    " 'swper_3': 9.999e+20,\n",
    " 'htsgwsfc': 9.999e+20,\n",
    " 'wdirsfc': 9.999e+20,\n",
    " 'swell_2': 9.999e+20,\n",
    " 'wvdirsfc': 9.999e+20,\n",
    " 'dirpwsfc': 9.999e+20,\n",
    " 'vgrdsfc': 9.999e+20,\n",
    " 'wvpersfc': 9.999e+20,\n",
    " 'swell_1': 10.44,\n",
    " 'swdir_1': 14.12,\n",
    " 'swdir_3': 216.47,\n",
    " 'swper_2': 17.56,\n",
    " 'swell_3': 274.38,\n",
    " 'windsfc': 5.19,\n",
    " 'wvhgtsfc': 11.759999}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4ee5275c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272.83408"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(f) * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "81ab695e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136.41711999999998"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(np.array(ftest)) * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3e85949e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'perpwsfc': 277.29,\n",
       " 'swdir_2': 5.19,\n",
       " 'swper_1': 11.759999,\n",
       " 'ugrdsfc': 9.999e+20,\n",
       " 'swper_3': 9.999e+20,\n",
       " 'htsgwsfc': 9.999e+20,\n",
       " 'wdirsfc': 9.999e+20,\n",
       " 'swell_2': 9.999e+20,\n",
       " 'wvdirsfc': 9.999e+20,\n",
       " 'dirpwsfc': 9.999e+20,\n",
       " 'vgrdsfc': 9.999e+20,\n",
       " 'wvpersfc': 9.999e+20,\n",
       " 'swell_1': 10.44,\n",
       " 'swdir_1': 14.12,\n",
       " 'swdir_3': 216.47,\n",
       " 'swper_2': 17.56,\n",
       " 'swell_3': 274.38,\n",
       " 'windsfc': 5.19,\n",
       " 'wvhgtsfc': 11.759999}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[0]['100']['400']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18a56f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[0])#[-1][45][70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "959c391f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1, 2.1, 3.1],\n",
       "       [1.2, 2.2, 3.2]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1.1,2.1,3.1],dtype=float)\n",
    "b = np.array([1.2,2.2,3.2], dtype=float)\n",
    "c = np.array([a,b], dtype=float)\n",
    "c\n",
    "# np.array(, a, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "382af35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3910.61968"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sys.getsizeof(f) * 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2d6020a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d284c18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "(2, 3)\n",
      "(2, 1, 3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[1.],\n",
       "         [2.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[1.],\n",
       "         [2.],\n",
       "         [3.]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "print(np.array([a,b]).shape)\n",
    "\n",
    "\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "print(np.array([a,b]).shape)\n",
    "\n",
    "a = [[[1],[2],[3]]]\n",
    "b = [[[1],[2],[3]]]\n",
    "print(np.array([a,b]).shape)\n",
    "np.array([a,b], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5bdf31b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.999e+20, 9.999e+20, 9.999e+20, ..., 9.999e+20, 9.999e+20,\n",
       "       9.999e+20], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data)[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.Pipeline.sat_dat import Satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4a10f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dumb = Satellite() # failed on timestep 79, 82\n",
    "second_test = dumb.build_tensor(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert interesting data (wavedir, waveheight) to lat/long grid, convert nulls to 0s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
